[2025-04-04T13:20:40.143+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Weather_Proj.Spark_Processing scheduled__2025-04-04T12:40:00+00:00 [queued]>
[2025-04-04T13:20:40.150+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Weather_Proj.Spark_Processing scheduled__2025-04-04T12:40:00+00:00 [queued]>
[2025-04-04T13:20:40.151+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2025-04-04T13:20:40.164+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): Spark_Processing> on 2025-04-04 12:40:00+00:00
[2025-04-04T13:20:40.169+0000] {standard_task_runner.py:60} INFO - Started process 6740 to run task
[2025-04-04T13:20:40.172+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'Weather_Proj', 'Spark_Processing', 'scheduled__2025-04-04T12:40:00+00:00', '--job-id', '211', '--raw', '--subdir', 'DAGS_FOLDER/Project_tasks.py', '--cfg-path', '/tmp/tmprrg6f4ql']
[2025-04-04T13:20:40.174+0000] {standard_task_runner.py:88} INFO - Job 211: Subtask Spark_Processing
[2025-04-04T13:20:40.214+0000] {task_command.py:423} INFO - Running <TaskInstance: Weather_Proj.Spark_Processing scheduled__2025-04-04T12:40:00+00:00 [running]> on host 7fc40170f80d
[2025-04-04T13:20:40.285+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Weather_Proj' AIRFLOW_CTX_TASK_ID='Spark_Processing' AIRFLOW_CTX_EXECUTION_DATE='2025-04-04T12:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-04T12:40:00+00:00'
[2025-04-04T13:20:46.010+0000] {logging_mixin.py:188} INFO - root
 |-- data: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- base: string (nullable = true)
 |    |    |-- clouds: struct (nullable = true)
 |    |    |    |-- all: long (nullable = true)
 |    |    |-- cod: long (nullable = true)
 |    |    |-- coord: struct (nullable = true)
 |    |    |    |-- lat: double (nullable = true)
 |    |    |    |-- lon: double (nullable = true)
 |    |    |-- dt: long (nullable = true)
 |    |    |-- id: long (nullable = true)
 |    |    |-- main: struct (nullable = true)
 |    |    |    |-- feels_like: double (nullable = true)
 |    |    |    |-- grnd_level: long (nullable = true)
 |    |    |    |-- humidity: long (nullable = true)
 |    |    |    |-- pressure: long (nullable = true)
 |    |    |    |-- sea_level: long (nullable = true)
 |    |    |    |-- temp: double (nullable = true)
 |    |    |    |-- temp_max: double (nullable = true)
 |    |    |    |-- temp_min: double (nullable = true)
 |    |    |-- name: string (nullable = true)
 |    |    |-- sys: struct (nullable = true)
 |    |    |    |-- country: string (nullable = true)
 |    |    |    |-- id: long (nullable = true)
 |    |    |    |-- sunrise: long (nullable = true)
 |    |    |    |-- sunset: long (nullable = true)
 |    |    |    |-- type: long (nullable = true)
 |    |    |-- timezone: long (nullable = true)
 |    |    |-- visibility: long (nullable = true)
 |    |    |-- weather: array (nullable = true)
 |    |    |    |-- element: struct (containsNull = true)
 |    |    |    |    |-- description: string (nullable = true)
 |    |    |    |    |-- icon: string (nullable = true)
 |    |    |    |    |-- id: long (nullable = true)
 |    |    |    |    |-- main: string (nullable = true)
 |    |    |-- wind: struct (nullable = true)
 |    |    |    |-- deg: long (nullable = true)
 |    |    |    |-- gust: double (nullable = true)
 |    |    |    |-- speed: double (nullable = true)
[2025-04-04T13:20:46.138+0000] {logging_mixin.py:188} INFO - root
 |-- city_name: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- city_timezone: long (nullable = true)
 |-- time_utc: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- city_country: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- sunrise_time_utc: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- sunset_time_utc: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- sea_level_meter: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- temp_celisus: array (nullable = true)
 |    |-- element: double (containsNull = true)
 |-- temp_min_celisus: array (nullable = true)
 |    |-- element: double (containsNull = true)
 |-- temp_max_celisus: array (nullable = true)
 |    |-- element: double (containsNull = true)
 |-- humidity_gram_m3: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- pressure_pascal: array (nullable = true)
 |    |-- element: long (containsNull = true)
 |-- latitude: array (nullable = true)
 |    |-- element: double (containsNull = true)
 |-- longtiude: array (nullable = true)
 |    |-- element: double (containsNull = true)
 |-- weather_description: array (nullable = true)
 |    |-- element: string (containsNull = true)
[2025-04-04T13:20:46.154+0000] {java_gateway.py:2273} INFO - Callback Server Starting
[2025-04-04T13:20:46.155+0000] {java_gateway.py:2275} INFO - Socket listening on ('127.0.0.1', 42807)
[2025-04-04T13:20:48.169+0000] {clientserver.py:561} INFO - Python Server ready to receive messages
[2025-04-04T13:20:48.170+0000] {clientserver.py:575} INFO - Received command c on object id p0
[2025-04-04T13:20:48.174+0000] {logging_mixin.py:188} INFO - Stuck here2 with batch id12
[2025-04-04T13:20:50.835+0000] {logging_mixin.py:188} INFO - +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+
|city_name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |city_timezone|time_utc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |city_country                                                                                                                                                                                        |sunrise_time_utc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |sunset_time_utc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |sea_level_meter                                                                                                                                                                                                                                                                                       |temp_celisus                                                                                                                                                                                                                                                                                                                                      |temp_min_celisus                                                                                                                                                                                                                                                                                                                                 |temp_max_celisus                                                                                                                                                                                                                                                                                                                                  |humidity_gram_m3                                                                                                                                                                                   |pressure_pascal                                                                                                                                                                                                                                                                                       |latitude                                                                                                                                                                                                                                                                                                                                                                                              |longtiude                                                                                                                                                                                                                                                                                                                                                                                                |weather_description|
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+
|[Sohag, Sīwah, Shibîn el-Qanâṭir, Sharm el Sheikh, North Sinai Governorate, Quwaysinā, Qena Governorate, Qaşr al Farāfirah, Qalyub, Matruh Governorate, Marsá Maţrūḩ, Damietta Governorate, Damanhur, Port Said, Safaga, Bilqās, Beni Suweif Governorate, Banhā, Zagazig, Asyut Governorate, Aswan Governorate, Aswān, Suez, Sharqia Province, Al Wāsiţah, New Valley Governorate, Luxor, El Qoseir, Al Qanāţir al Khayrīyah, Qalyubia Governorate, Cairo, Minya Governorate, Monufia Governorate, Al Maţarīyah, Al Mansurah, Al Maḩallah al Kubrá, Giza, Ismailia Governorate, Alexandria, Hurghada, Gharbia Governorate, Faiyum Governorate, Beheira, Red Sea, El Gouna, Makadi Bay, Port el Ghalib, Naama Bay, Ain Sukhna]|7200         |[1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171]|[EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG]|[1743738047, 1743739441, 1743738012, 1743737379, 1743737434, 1743738042, 1743737818, 1743738924, 1743738044, 1743738949, 1743738952, 1743737872, 1743738190, 1743737747, 1743737504, 1743737971, 1743738233, 1743738040, 1743737958, 1743738169, 1743737860, 1743737834, 1743737734, 1743737883, 1743738073, 1743738773, 1743737847, 1743737442, 1743738060, 1743738029, 1743738037, 1743738403, 1743738082, 1743737811, 1743737972, 1743738026, 1743738048, 1743737743, 1743738307, 1743737517, 1743738068, 1743738243, 1743738270, 1743737525, 1743737544, 1743737504, 1743737513, 1743737371, 1743737797]|[1743783075, 1743784645, 1743783290, 1743782490, 1743782725, 1743783338, 1743782798, 1743783985, 1743783313, 1743784186, 1743784305, 1743783224, 1743783520, 1743783092, 1743782542, 1743783313, 1743783415, 1743783329, 1743783256, 1743783239, 1743782684, 1743782707, 1743782989, 1743783189, 1743783284, 1743783690, 1743782820, 1743782440, 1743783330, 1743783307, 1743783299, 1743783532, 1743783374, 1743783150, 1743783301, 1743783351, 1743783306, 1743783041, 1743783650, 1743782589, 1743783388, 1743783452, 1743783569, 1743782593, 1743782626, 1743782560, 1743782586, 1743782487, 1743783027]|[1011, 1012, 1011, 1008, 1012, 1011, 1010, 1012, 1012, 1013, 1015, 1013, 1013, 1014, 1008, 1013, 1012, 1011, 1011, 1011, 1010, 1009, 1012, 1012, 1012, 1012, 1010, 1010, 1012, 1011, 1012, 1012, 1011, 1013, 1012, 1012, 1012, 1012, 1014, 1009, 1012, 1012, 1013, 1009, 1009, 1008, 1009, 1008, 1012]|[30.08, 26.83, 23.04, 29.08, 21.8, 27.11, 32.08, 29.44, 23.03, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.04, 23.09, 23.42, 27.09, 26.87, 18.57, 24.92, 24.35, 23.41, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[30.08, 26.83, 23.04, 29.08, 21.8, 27.11, 32.08, 29.44, 23.03, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.04, 23.09, 23.01, 27.09, 26.87, 18.57, 24.92, 24.35, 23.0, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[30.08, 26.83, 23.45, 29.08, 21.8, 27.11, 32.08, 29.44, 23.44, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.45, 23.49, 23.42, 27.09, 26.87, 18.57, 24.92, 24.35, 23.41, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[15, 22, 22, 26, 36, 21, 11, 14, 35, 25, 58, 52, 35, 72, 41, 33, 22, 21, 21, 16, 26, 9, 25, 35, 19, 12, 13, 41, 23, 22, 35, 20, 22, 66, 26, 31, 22, 27, 61, 35, 29, 21, 32, 30, 35, 46, 39, 34, 29]|[1011, 1012, 1011, 1008, 1012, 1011, 1010, 1012, 1012, 1013, 1015, 1013, 1013, 1014, 1008, 1013, 1012, 1011, 1011, 1011, 1010, 1009, 1012, 1012, 1012, 1012, 1010, 1010, 1012, 1011, 1012, 1012, 1011, 1013, 1012, 1012, 1012, 1012, 1014, 1009, 1012, 1012, 1013, 1009, 1009, 1008, 1009, 1008, 1012]|[31.6948, 25.5195, 31.3202, 34.305, 33.7, 31.1578, 32.75, 27.9698, 31.2056, 27.5, 27.2453, 31.75, 30.4691, 32.2841, 33.9365, 31.358, 30.6, 31.1786, 31.502, 31.1, 32.9, 32.907, 32.5263, 31.8, 31.2056, 28.9, 32.6421, 34.2779, 31.137, 31.25, 31.2497, 30.0, 31.0, 32.0311, 31.3807, 31.165, 31.2109, 32.4, 29.9553, 33.8129, 31.0, 30.5, 30.2, 33.7884, 33.6782, 33.8995, 33.8268, 34.3295, 32.3167]|[26.557, 29.2041, 30.3127, 27.8518, 30.5, 30.5648, 25.8, 27.0568, 30.1792, 29.7, 31.3525, 31.35, 31.0392, 31.2565, 26.7292, 31.2145, 28.9, 30.4591, 30.5877, 27.2, 23.3, 24.0934, 29.9737, 30.7, 29.3378, 24.8, 25.6989, 26.1043, 30.1933, 30.3, 30.0626, 28.1, 30.5, 31.1829, 31.0364, 30.9745, 30.0081, 30.6, 31.2156, 27.2574, 30.9, 29.3, 30.6, 27.1862, 27.3942, 26.9912, 27.2543, 27.9138, 29.6002]|[clear sky]        |
|[Sohag, Sīwah, Shibîn el-Qanâṭir, Sharm el Sheikh, North Sinai Governorate, Quwaysinā, Qena Governorate, Qaşr al Farāfirah, Qalyub, Matruh Governorate, Marsá Maţrūḩ, Damietta Governorate, Damanhur, Port Said, Safaga, Bilqās, Beni Suweif Governorate, Banhā, Zagazig, Asyut Governorate, Aswan Governorate, Aswān, Suez, Sharqia Province, Al Wāsiţah, New Valley Governorate, Luxor, El Qoseir, Al Qanāţir al Khayrīyah, Qalyubia Governorate, Cairo, Minya Governorate, Monufia Governorate, Al Maţarīyah, Al Mansurah, Al Maḩallah al Kubrá, Giza, Ismailia Governorate, Alexandria, Hurghada, Gharbia Governorate, Faiyum Governorate, Beheira, Red Sea, El Gouna, Makadi Bay, Port el Ghalib, Naama Bay, Ain Sukhna]|7200         |[1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171]|[EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG]|[1743738047, 1743739441, 1743738012, 1743737379, 1743737434, 1743738042, 1743737818, 1743738924, 1743738044, 1743738949, 1743738952, 1743737872, 1743738190, 1743737747, 1743737504, 1743737971, 1743738233, 1743738040, 1743737958, 1743738169, 1743737860, 1743737834, 1743737734, 1743737883, 1743738073, 1743738773, 1743737847, 1743737442, 1743738060, 1743738029, 1743738037, 1743738403, 1743738082, 1743737811, 1743737972, 1743738026, 1743738048, 1743737743, 1743738307, 1743737517, 1743738068, 1743738243, 1743738270, 1743737525, 1743737544, 1743737504, 1743737513, 1743737371, 1743737797]|[1743783075, 1743784645, 1743783290, 1743782490, 1743782725, 1743783338, 1743782798, 1743783985, 1743783313, 1743784186, 1743784305, 1743783224, 1743783520, 1743783092, 1743782542, 1743783313, 1743783415, 1743783329, 1743783256, 1743783239, 1743782684, 1743782707, 1743782989, 1743783189, 1743783284, 1743783690, 1743782820, 1743782440, 1743783330, 1743783307, 1743783299, 1743783532, 1743783374, 1743783150, 1743783301, 1743783351, 1743783306, 1743783041, 1743783650, 1743782589, 1743783388, 1743783452, 1743783569, 1743782593, 1743782626, 1743782560, 1743782586, 1743782487, 1743783027]|[1011, 1012, 1011, 1008, 1012, 1011, 1010, 1012, 1012, 1013, 1015, 1013, 1013, 1014, 1008, 1013, 1012, 1011, 1011, 1011, 1010, 1009, 1012, 1012, 1012, 1012, 1010, 1010, 1012, 1011, 1012, 1012, 1011, 1013, 1012, 1012, 1012, 1012, 1014, 1009, 1012, 1012, 1013, 1009, 1009, 1008, 1009, 1008, 1012]|[30.08, 26.83, 23.04, 29.08, 21.8, 27.11, 32.08, 29.44, 23.03, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.04, 23.09, 23.42, 27.09, 26.87, 18.57, 24.92, 24.35, 23.41, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[30.08, 26.83, 23.04, 29.08, 21.8, 27.11, 32.08, 29.44, 23.03, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.04, 23.09, 23.01, 27.09, 26.87, 18.57, 24.92, 24.35, 23.0, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[30.08, 26.83, 23.45, 29.08, 21.8, 27.11, 32.08, 29.44, 23.44, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.45, 23.49, 23.42, 27.09, 26.87, 18.57, 24.92, 24.35, 23.41, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[15, 22, 22, 26, 36, 21, 11, 14, 35, 25, 58, 52, 35, 72, 41, 33, 22, 21, 21, 16, 26, 9, 25, 35, 19, 12, 13, 41, 23, 22, 35, 20, 22, 66, 26, 31, 22, 27, 61, 35, 29, 21, 32, 30, 35, 46, 39, 34, 29]|[1011, 1012, 1011, 1008, 1012, 1011, 1010, 1012, 1012, 1013, 1015, 1013, 1013, 1014, 1008, 1013, 1012, 1011, 1011, 1011, 1010, 1009, 1012, 1012, 1012, 1012, 1010, 1010, 1012, 1011, 1012, 1012, 1011, 1013, 1012, 1012, 1012, 1012, 1014, 1009, 1012, 1012, 1013, 1009, 1009, 1008, 1009, 1008, 1012]|[31.6948, 25.5195, 31.3202, 34.305, 33.7, 31.1578, 32.75, 27.9698, 31.2056, 27.5, 27.2453, 31.75, 30.4691, 32.2841, 33.9365, 31.358, 30.6, 31.1786, 31.502, 31.1, 32.9, 32.907, 32.5263, 31.8, 31.2056, 28.9, 32.6421, 34.2779, 31.137, 31.25, 31.2497, 30.0, 31.0, 32.0311, 31.3807, 31.165, 31.2109, 32.4, 29.9553, 33.8129, 31.0, 30.5, 30.2, 33.7884, 33.6782, 33.8995, 33.8268, 34.3295, 32.3167]|[26.557, 29.2041, 30.3127, 27.8518, 30.5, 30.5648, 25.8, 27.0568, 30.1792, 29.7, 31.3525, 31.35, 31.0392, 31.2565, 26.7292, 31.2145, 28.9, 30.4591, 30.5877, 27.2, 23.3, 24.0934, 29.9737, 30.7, 29.3378, 24.8, 25.6989, 26.1043, 30.1933, 30.3, 30.0626, 28.1, 30.5, 31.1829, 31.0364, 30.9745, 30.0081, 30.6, 31.2156, 27.2574, 30.9, 29.3, 30.6, 27.1862, 27.3942, 26.9912, 27.2543, 27.9138, 29.6002]|[clear sky]        |
|[Sohag, Sīwah, Shibîn el-Qanâṭir, Sharm el Sheikh, North Sinai Governorate, Quwaysinā, Qena Governorate, Qaşr al Farāfirah, Qalyub, Matruh Governorate, Marsá Maţrūḩ, Damietta Governorate, Damanhur, Port Said, Safaga, Bilqās, Beni Suweif Governorate, Banhā, Zagazig, Asyut Governorate, Aswan Governorate, Aswān, Suez, Sharqia Province, Al Wāsiţah, New Valley Governorate, Luxor, El Qoseir, Al Qanāţir al Khayrīyah, Qalyubia Governorate, Cairo, Minya Governorate, Monufia Governorate, Al Maţarīyah, Al Mansurah, Al Maḩallah al Kubrá, Giza, Ismailia Governorate, Alexandria, Hurghada, Gharbia Governorate, Faiyum Governorate, Beheira, Red Sea, El Gouna, Makadi Bay, Port el Ghalib, Naama Bay, Ain Sukhna]|7200         |[1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171]|[EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG, EG]|[1743738047, 1743739441, 1743738012, 1743737379, 1743737434, 1743738042, 1743737818, 1743738924, 1743738044, 1743738949, 1743738952, 1743737872, 1743738190, 1743737747, 1743737504, 1743737971, 1743738233, 1743738040, 1743737958, 1743738169, 1743737860, 1743737834, 1743737734, 1743737883, 1743738073, 1743738773, 1743737847, 1743737442, 1743738060, 1743738029, 1743738037, 1743738403, 1743738082, 1743737811, 1743737972, 1743738026, 1743738048, 1743737743, 1743738307, 1743737517, 1743738068, 1743738243, 1743738270, 1743737525, 1743737544, 1743737504, 1743737513, 1743737371, 1743737797]|[1743783075, 1743784645, 1743783290, 1743782490, 1743782725, 1743783338, 1743782798, 1743783985, 1743783313, 1743784186, 1743784305, 1743783224, 1743783520, 1743783092, 1743782542, 1743783313, 1743783415, 1743783329, 1743783256, 1743783239, 1743782684, 1743782707, 1743782989, 1743783189, 1743783284, 1743783690, 1743782820, 1743782440, 1743783330, 1743783307, 1743783299, 1743783532, 1743783374, 1743783150, 1743783301, 1743783351, 1743783306, 1743783041, 1743783650, 1743782589, 1743783388, 1743783452, 1743783569, 1743782593, 1743782626, 1743782560, 1743782586, 1743782487, 1743783027]|[1011, 1012, 1011, 1008, 1012, 1011, 1010, 1012, 1012, 1013, 1015, 1013, 1013, 1014, 1008, 1013, 1012, 1011, 1011, 1011, 1010, 1009, 1012, 1012, 1012, 1012, 1010, 1010, 1012, 1011, 1012, 1012, 1011, 1013, 1012, 1012, 1012, 1012, 1014, 1009, 1012, 1012, 1013, 1009, 1009, 1008, 1009, 1008, 1012]|[30.08, 26.83, 23.04, 29.08, 21.8, 27.11, 32.08, 29.44, 23.03, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.04, 23.09, 23.42, 27.09, 26.87, 18.57, 24.92, 24.35, 23.41, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[30.08, 26.83, 23.04, 29.08, 21.8, 27.11, 32.08, 29.44, 23.03, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.04, 23.09, 23.01, 27.09, 26.87, 18.57, 24.92, 24.35, 23.0, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[30.08, 26.83, 23.45, 29.08, 21.8, 27.11, 32.08, 29.44, 23.44, 25.77, 18.89, 20.38, 23.51, 17.9, 26.27, 23.35, 25.45, 27.11, 27.22, 29.43, 26.26, 32.77, 25.39, 24.41, 26.36, 29.87, 32.03, 25.28, 23.45, 23.49, 23.42, 27.09, 26.87, 18.57, 24.92, 24.35, 23.41, 24.77, 19.1, 25.85, 24.82, 26.36, 24.9, 26.91, 26.33, 25.2, 25.23, 26.37, 24.04]|[15, 22, 22, 26, 36, 21, 11, 14, 35, 25, 58, 52, 35, 72, 41, 33, 22, 21, 21, 16, 26, 9, 25, 35, 19, 12, 13, 41, 23, 22, 35, 20, 22, 66, 26, 31, 22, 27, 61, 35, 29, 21, 32, 30, 35, 46, 39, 34, 29]|[1011, 1012, 1011, 1008, 1012, 1011, 1010, 1012, 1012, 1013, 1015, 1013, 1013, 1014, 1008, 1013, 1012, 1011, 1011, 1011, 1010, 1009, 1012, 1012, 1012, 1012, 1010, 1010, 1012, 1011, 1012, 1012, 1011, 1013, 1012, 1012, 1012, 1012, 1014, 1009, 1012, 1012, 1013, 1009, 1009, 1008, 1009, 1008, 1012]|[31.6948, 25.5195, 31.3202, 34.305, 33.7, 31.1578, 32.75, 27.9698, 31.2056, 27.5, 27.2453, 31.75, 30.4691, 32.2841, 33.9365, 31.358, 30.6, 31.1786, 31.502, 31.1, 32.9, 32.907, 32.5263, 31.8, 31.2056, 28.9, 32.6421, 34.2779, 31.137, 31.25, 31.2497, 30.0, 31.0, 32.0311, 31.3807, 31.165, 31.2109, 32.4, 29.9553, 33.8129, 31.0, 30.5, 30.2, 33.7884, 33.6782, 33.8995, 33.8268, 34.3295, 32.3167]|[26.557, 29.2041, 30.3127, 27.8518, 30.5, 30.5648, 25.8, 27.0568, 30.1792, 29.7, 31.3525, 31.35, 31.0392, 31.2565, 26.7292, 31.2145, 28.9, 30.4591, 30.5877, 27.2, 23.3, 24.0934, 29.9737, 30.7, 29.3378, 24.8, 25.6989, 26.1043, 30.1933, 30.3, 30.0626, 28.1, 30.5, 31.1829, 31.0364, 30.9745, 30.0081, 30.6, 31.2156, 27.2574, 30.9, 29.3, 30.6, 27.1862, 27.3942, 26.9912, 27.2543, 27.9138, 29.6002]|[clear sky]        |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+
only showing top 3 rows
[2025-04-04T13:20:52.415+0000] {logging_mixin.py:188} INFO - ✅ Batch 2025-04-04 13:20:51.490074 has data: 4802 rows
[2025-04-04T13:20:56.306+0000] {clientserver.py:621} ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/airflow/dags/WeatherScripts/spark_Weather.py", line 184, in write_to_cassandra
    .save()
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1461, in save
    self._jwrite.save()
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o113.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (7fc40170f80d executor driver): com.datastax.spark.connector.types.TypeConversionException: Cannot convert object WrappedArray(1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171) of type class scala.collection.mutable.WrappedArray$ofRef to Long.
	at com.datastax.spark.connector.types.TypeConverter.$anonfun$convert$1(TypeConverter.scala:46)
	at scala.PartialFunction$AndThen.applyOrElse(PartialFunction.scala:195)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter$$anonfun$convertPF$48.applyOrElse(TypeConverter.scala:902)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.datasource.InternalRowWriter.$anonfun$converters$2(InternalRowWriterFactory.scala:43)
	at scala.Function1.$anonfun$andThen$1(Function1.scala:57)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:54)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:24)
	at com.datastax.spark.connector.writer.BoundStatementBuilder.bind(BoundStatementBuilder.scala:102)
	at com.datastax.spark.connector.writer.GroupingBatchBuilderBase.batchRecord(GroupingBatchBuilder.scala:140)
	at com.datastax.spark.connector.writer.AsyncStatementWriter.write(TableWriter.scala:264)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:43)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:29)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:498)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:453)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:230)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:342)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:315)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: com.datastax.spark.connector.types.TypeConversionException: Cannot convert object WrappedArray(1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171) of type class scala.collection.mutable.WrappedArray$ofRef to Long.
	at com.datastax.spark.connector.types.TypeConverter.$anonfun$convert$1(TypeConverter.scala:46)
	at scala.PartialFunction$AndThen.applyOrElse(PartialFunction.scala:195)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter$$anonfun$convertPF$48.applyOrElse(TypeConverter.scala:902)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.datasource.InternalRowWriter.$anonfun$converters$2(InternalRowWriterFactory.scala:43)
	at scala.Function1.$anonfun$andThen$1(Function1.scala:57)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:54)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:24)
	at com.datastax.spark.connector.writer.BoundStatementBuilder.bind(BoundStatementBuilder.scala:102)
	at com.datastax.spark.connector.writer.GroupingBatchBuilderBase.batchRecord(GroupingBatchBuilder.scala:140)
	at com.datastax.spark.connector.writer.AsyncStatementWriter.write(TableWriter.scala:264)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:43)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:29)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:498)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:453)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

[2025-04-04T13:20:56.605+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/WeatherScripts/spark_Weather.py", line 161, in processData
    query.awaitTermination(60)
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/streaming/query.py", line 219, in awaitTermination
    return self._jsq.awaitTermination(int(timeout * 1000))
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 9d7c8570-6a84-4697-b823-80ee3b6966cb, runId = 135be0d9-8bda-4ced-aaef-e0d49f7f05f5] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/airflow/dags/WeatherScripts/spark_Weather.py", line 184, in write_to_cassandra
    .save()
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1461, in save
    self._jwrite.save()
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o113.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (7fc40170f80d executor driver): com.datastax.spark.connector.types.TypeConversionException: Cannot convert object WrappedArray(1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171) of type class scala.collection.mutable.WrappedArray$ofRef to Long.
	at com.datastax.spark.connector.types.TypeConverter.$anonfun$convert$1(TypeConverter.scala:46)
	at scala.PartialFunction$AndThen.applyOrElse(PartialFunction.scala:195)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter$$anonfun$convertPF$48.applyOrElse(TypeConverter.scala:902)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.datasource.InternalRowWriter.$anonfun$converters$2(InternalRowWriterFactory.scala:43)
	at scala.Function1.$anonfun$andThen$1(Function1.scala:57)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:54)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:24)
	at com.datastax.spark.connector.writer.BoundStatementBuilder.bind(BoundStatementBuilder.scala:102)
	at com.datastax.spark.connector.writer.GroupingBatchBuilderBase.batchRecord(GroupingBatchBuilder.scala:140)
	at com.datastax.spark.connector.writer.AsyncStatementWriter.write(TableWriter.scala:264)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:43)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:29)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:498)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:453)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:230)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:342)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:315)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: com.datastax.spark.connector.types.TypeConversionException: Cannot convert object WrappedArray(1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171) of type class scala.collection.mutable.WrappedArray$ofRef to Long.
	at com.datastax.spark.connector.types.TypeConverter.$anonfun$convert$1(TypeConverter.scala:46)
	at scala.PartialFunction$AndThen.applyOrElse(PartialFunction.scala:195)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter$$anonfun$convertPF$48.applyOrElse(TypeConverter.scala:902)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.datasource.InternalRowWriter.$anonfun$converters$2(InternalRowWriterFactory.scala:43)
	at scala.Function1.$anonfun$andThen$1(Function1.scala:57)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:54)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:24)
	at com.datastax.spark.connector.writer.BoundStatementBuilder.bind(BoundStatementBuilder.scala:102)
	at com.datastax.spark.connector.writer.GroupingBatchBuilderBase.batchRecord(GroupingBatchBuilder.scala:140)
	at com.datastax.spark.connector.writer.AsyncStatementWriter.write(TableWriter.scala:264)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:43)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:29)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:498)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:453)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)


[2025-04-04T13:20:56.632+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=Weather_Proj, task_id=Spark_Processing, execution_date=20250404T124000, start_date=20250404T132040, end_date=20250404T132056
[2025-04-04T13:20:56.645+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 211 for task Spark_Processing ([STREAM_FAILED] Query [id = 9d7c8570-6a84-4697-b823-80ee3b6966cb, runId = 135be0d9-8bda-4ced-aaef-e0d49f7f05f5] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/airflow/dags/WeatherScripts/spark_Weather.py", line 184, in write_to_cassandra
    .save()
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py", line 1461, in save
    self._jwrite.save()
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o113.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (7fc40170f80d executor driver): com.datastax.spark.connector.types.TypeConversionException: Cannot convert object WrappedArray(1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171) of type class scala.collection.mutable.WrappedArray$ofRef to Long.
	at com.datastax.spark.connector.types.TypeConverter.$anonfun$convert$1(TypeConverter.scala:46)
	at scala.PartialFunction$AndThen.applyOrElse(PartialFunction.scala:195)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter$$anonfun$convertPF$48.applyOrElse(TypeConverter.scala:902)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.datasource.InternalRowWriter.$anonfun$converters$2(InternalRowWriterFactory.scala:43)
	at scala.Function1.$anonfun$andThen$1(Function1.scala:57)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:54)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:24)
	at com.datastax.spark.connector.writer.BoundStatementBuilder.bind(BoundStatementBuilder.scala:102)
	at com.datastax.spark.connector.writer.GroupingBatchBuilderBase.batchRecord(GroupingBatchBuilder.scala:140)
	at com.datastax.spark.connector.writer.AsyncStatementWriter.write(TableWriter.scala:264)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:43)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:29)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:498)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:453)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:230)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:342)
	at org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:341)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:230)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:315)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: com.datastax.spark.connector.types.TypeConversionException: Cannot convert object WrappedArray(1743772153, 1743772154, 1743772154, 1743771897, 1743772155, 1743772156, 1743772156, 1743772156, 1743772157, 1743772157, 1743772157, 1743772158, 1743772158, 1743772159, 1743772159, 1743772159, 1743772160, 1743772160, 1743772160, 1743772161, 1743772161, 1743772161, 1743772057, 1743772162, 1743772162, 1743772163, 1743771938, 1743772164, 1743772164, 1743772164, 1743772029, 1743772165, 1743772165, 1743772166, 1743772166, 1743772166, 1743771872, 1743772167, 1743772167, 1743772168, 1743772168, 1743772168, 1743772169, 1743772068, 1743772170, 1743772148, 1743772170, 1743772171, 1743772171) of type class scala.collection.mutable.WrappedArray$ofRef to Long.
	at com.datastax.spark.connector.types.TypeConverter.$anonfun$convert$1(TypeConverter.scala:46)
	at scala.PartialFunction$AndThen.applyOrElse(PartialFunction.scala:195)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$JavaLongConverter$.convert(TypeConverter.scala:194)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter$$anonfun$convertPF$48.applyOrElse(TypeConverter.scala:902)
	at com.datastax.spark.connector.types.TypeConverter.convert(TypeConverter.scala:44)
	at com.datastax.spark.connector.types.TypeConverter.convert$(TypeConverter.scala:43)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.com$datastax$spark$connector$types$NullableTypeConverter$$super$convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert(TypeConverter.scala:57)
	at com.datastax.spark.connector.types.NullableTypeConverter.convert$(TypeConverter.scala:55)
	at com.datastax.spark.connector.types.TypeConverter$OptionToNullConverter.convert(TypeConverter.scala:885)
	at com.datastax.spark.connector.datasource.InternalRowWriter.$anonfun$converters$2(InternalRowWriterFactory.scala:43)
	at scala.Function1.$anonfun$andThen$1(Function1.scala:57)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:54)
	at com.datastax.spark.connector.datasource.InternalRowWriter.readColumnValues(InternalRowWriterFactory.scala:24)
	at com.datastax.spark.connector.writer.BoundStatementBuilder.bind(BoundStatementBuilder.scala:102)
	at com.datastax.spark.connector.writer.GroupingBatchBuilderBase.batchRecord(GroupingBatchBuilder.scala:140)
	at com.datastax.spark.connector.writer.AsyncStatementWriter.write(TableWriter.scala:264)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:43)
	at com.datastax.spark.connector.datasource.CassandraDriverDataWriter.write(CasssandraDriverDataWriterFactory.scala:29)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:498)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:453)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)
	at org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)
	at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)

; 6740)
[2025-04-04T13:20:56.684+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-04-04T13:20:56.699+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
