[2025-04-04T12:14:31.574+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Weather_Proj.Spark_Processing scheduled__2025-04-04T11:20:00+00:00 [queued]>
[2025-04-04T12:14:31.584+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Weather_Proj.Spark_Processing scheduled__2025-04-04T11:20:00+00:00 [queued]>
[2025-04-04T12:14:31.584+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2025-04-04T12:14:31.597+0000] {taskinstance.py:2191} INFO - Executing <Task(PythonOperator): Spark_Processing> on 2025-04-04 11:20:00+00:00
[2025-04-04T12:14:31.601+0000] {standard_task_runner.py:60} INFO - Started process 126 to run task
[2025-04-04T12:14:31.604+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'Weather_Proj', 'Spark_Processing', 'scheduled__2025-04-04T11:20:00+00:00', '--job-id', '153', '--raw', '--subdir', 'DAGS_FOLDER/Project_tasks.py', '--cfg-path', '/tmp/tmpd320tt_y']
[2025-04-04T12:14:31.606+0000] {standard_task_runner.py:88} INFO - Job 153: Subtask Spark_Processing
[2025-04-04T12:14:31.649+0000] {task_command.py:423} INFO - Running <TaskInstance: Weather_Proj.Spark_Processing scheduled__2025-04-04T11:20:00+00:00 [running]> on host 7fc40170f80d
[2025-04-04T12:14:31.713+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Weather_Proj' AIRFLOW_CTX_TASK_ID='Spark_Processing' AIRFLOW_CTX_EXECUTION_DATE='2025-04-04T11:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-04T11:20:00+00:00'
[2025-04-04T12:16:01.715+0000] {logging_mixin.py:188} INFO - root
 |-- data: struct (nullable = true)
 |    |-- base: string (nullable = true)
 |    |-- clouds: struct (nullable = true)
 |    |    |-- all: long (nullable = true)
 |    |-- cod: long (nullable = true)
 |    |-- coord: struct (nullable = true)
 |    |    |-- lat: double (nullable = true)
 |    |    |-- lon: double (nullable = true)
 |    |-- dt: long (nullable = true)
 |    |-- id: long (nullable = true)
 |    |-- main: struct (nullable = true)
 |    |    |-- feels_like: double (nullable = true)
 |    |    |-- grnd_level: long (nullable = true)
 |    |    |-- humidity: long (nullable = true)
 |    |    |-- pressure: long (nullable = true)
 |    |    |-- sea_level: long (nullable = true)
 |    |    |-- temp: double (nullable = true)
 |    |    |-- temp_max: double (nullable = true)
 |    |    |-- temp_min: double (nullable = true)
 |    |-- name: string (nullable = true)
 |    |-- sys: struct (nullable = true)
 |    |    |-- country: string (nullable = true)
 |    |    |-- id: long (nullable = true)
 |    |    |-- sunrise: long (nullable = true)
 |    |    |-- sunset: long (nullable = true)
 |    |    |-- type: long (nullable = true)
 |    |-- timezone: long (nullable = true)
 |    |-- visibility: long (nullable = true)
 |    |-- weather: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- description: string (nullable = true)
 |    |    |    |-- icon: string (nullable = true)
 |    |    |    |-- id: long (nullable = true)
 |    |    |    |-- main: string (nullable = true)
 |    |-- wind: struct (nullable = true)
 |    |    |-- deg: long (nullable = true)
 |    |    |-- gust: double (nullable = true)
 |    |    |-- speed: double (nullable = true)
[2025-04-04T12:16:01.833+0000] {logging_mixin.py:188} INFO - root
 |-- city_name: string (nullable = true)
 |-- city_timezone: long (nullable = true)
 |-- time_utc: long (nullable = true)
 |-- city_country: string (nullable = true)
 |-- sunrise_time_utc: long (nullable = true)
 |-- sunset_time_utc: long (nullable = true)
 |-- sea_level_meter: long (nullable = true)
 |-- temp_celisus: double (nullable = true)
 |-- temp_min_celisus: double (nullable = true)
 |-- temp_max_celisus: double (nullable = true)
 |-- humidity_gram_m3: long (nullable = true)
 |-- pressure_pascal: long (nullable = true)
 |-- latitude: double (nullable = true)
 |-- longtiude: double (nullable = true)
 |-- weather_description: string (nullable = true)
[2025-04-04T12:16:01.849+0000] {java_gateway.py:2273} INFO - Callback Server Starting
[2025-04-04T12:16:01.850+0000] {java_gateway.py:2275} INFO - Socket listening on ('127.0.0.1', 46547)
[2025-04-04T12:16:06.118+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 199, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py", line 216, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/WeatherScripts/spark_Weather.py", line 150, in processData
    query.awaitTermination(60)
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/sql/streaming/query.py", line 219, in awaitTermination
    return self._jsq.awaitTermination(int(timeout * 1000))
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/home/airflow/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 9d7c8570-6a84-4697-b823-80ee3b6966cb, runId = 6e1ccb84-3d9d-41fe-9ffb-965cb316eee1] terminated with exception: Partition Topic_1-0's offset was changed from 13 to 2, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you don't want your streaming query to fail on such cases, set the
 source option "failOnDataLoss" to "false".
    
[2025-04-04T12:16:06.141+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=Weather_Proj, task_id=Spark_Processing, execution_date=20250404T112000, start_date=20250404T121431, end_date=20250404T121606
[2025-04-04T12:16:06.162+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 153 for task Spark_Processing ([STREAM_FAILED] Query [id = 9d7c8570-6a84-4697-b823-80ee3b6966cb, runId = 6e1ccb84-3d9d-41fe-9ffb-965cb316eee1] terminated with exception: Partition Topic_1-0's offset was changed from 13 to 2, some data may have been missed. 
Some data may have been lost because they are not available in Kafka any more; either the
 data was aged out by Kafka or the topic may have been deleted before all the data in the
 topic was processed. If you don't want your streaming query to fail on such cases, set the
 source option "failOnDataLoss" to "false".
    ; 126)
[2025-04-04T12:16:06.204+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-04-04T12:16:06.229+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
